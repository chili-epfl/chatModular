{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalized Forum Scraping"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Works for Title, Link, Username in Holiday Truths                but only for one page (no Next)\n",
    "          Title, Link, Username in Wrong Planet                  but is slow\n",
    "          Title, Link           in Stack Overflow                but only for one page (cant reach next)   TRY AGAIN!\n",
    "          Title                 in Au Féminin                    but only for one page\n",
    "          Title, Link, Username in Trip Advisor                  but only for one page (gui-spriteNext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be careful to run the cells in order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that finds the tags allowing to retrieve the data we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tag(x, comp, tags):\n",
    "    tag = x.name\n",
    "    try:\n",
    "        class_name = x['class'][0]\n",
    "        tags.append(class_name)\n",
    "        tags.append(tag)\n",
    "        next_ = x.findNext(tag, {'class': class_name})\n",
    "    except:\n",
    "        #print(\"TAG: \", tag)\n",
    "        tags.append(tag)\n",
    "        #print(\"TAGS: \", tags)\n",
    "        next_ = x.findNext(tag)\n",
    "        #print(\"next: \", next_)\n",
    "        \n",
    "    if comp in next_.text:\n",
    "        print(\"I've found the right tag, it's \", tags[::-1])\n",
    "        return tags[::-1]\n",
    "    else:\n",
    "        return find_tag(x.parent, comp, tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_next_tag():\n",
    "    if soup.find(text = \"Next\") != None:\n",
    "        next_tag = soup.find(text = \"Next\").parent.name\n",
    "        print(\"The tag for next page is \", next_tag)\n",
    "        return next_tag\n",
    "    elif soup.find(text = \"next\") != None:\n",
    "        next_tag = soup.find(text = \"next\").parent.name\n",
    "        print(\"The tag for next page is \", next_tag)\n",
    "        return next_tag\n",
    "    else:\n",
    "        print(\"tag of the next page is not treated in this program\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_next(soup, next_tag):\n",
    "    try:\n",
    "        #check if find instead of find_all always work if not do [0]['href'][1:]\n",
    "        if next_tag is not None:\n",
    "            return soup.find_all(next_tag, string=\"Next\")[0]['href'][1:]       #also work with \"next\"\n",
    "        return \"\"\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def collect_forum_data(soup_row, tags, soup_row_ok = 0, link=\"\"):\n",
    "    \n",
    "    data = {}\n",
    "    try:\n",
    "        if not soup_row_ok:         #change with length\n",
    "            soup_row = soup_row.find(tags[len(tags) -1])\n",
    "    \n",
    "        data['Title'] = soup_row.text.strip()\n",
    "        data['Link'] = soup_row['href'][1:]\n",
    "        \n",
    "    except:\n",
    "        pass\n",
    "    return data\n",
    "\n",
    "def collect_post_data(soup_row, tags, soup_row_ok, link=\"\"):\n",
    "    data = {}\n",
    "    try:          #check case tags==3, 2\n",
    "        if len(tags) == 2 or len(tags) == 3:            #case where len == 2 and no class will fail (should do a second fin)            \n",
    "            soup_row = soup_row.find(tags[len(tags)-1])\n",
    "        elif len(tags) == 4:\n",
    "            soup_row = soup_row.find(tags[2]).find(tags[3])    \n",
    "        \n",
    "        data['Link'] = link.replace(PREFIX_URL, '')\n",
    "\n",
    "        try:\n",
    "            data['Username'] = soup_row.text.strip()\n",
    "        except:\n",
    "            pass\n",
    "    except:\n",
    "        pass\n",
    "    return data\n",
    "\n",
    "def collect_recursively(data, soup, tags, next_tag, fcte_name, link=\"\", index=\"\"):\n",
    "    try:\n",
    "        if index:\n",
    "            print(index, end='\\r', flush=True)\n",
    "        \n",
    "        soup_row_ok = 0\n",
    "        if len(tags) == 2:\n",
    "            soup_rows = soup.find_all(tags[0], {'class': tags[1]})\n",
    "            if not soup_rows:\n",
    "                soup_rows = soup.find_all(tags[0]) \n",
    "            else:\n",
    "                soup_row_ok = 1      \n",
    "        if len(tags) == 3 or len(tags) == 4:\n",
    "            soup_rows = soup.find_all(tags[0], {'class': tags[1]})\n",
    "    \n",
    "        \n",
    "        #print(soup_rows)\n",
    "        data.extend([fcte_name(soup_row, tags, soup_row_ok, link) for soup_row in soup_rows])\n",
    "        next_url = find_next(soup, next_tag)           #put a condition (something with next_tag == None)\n",
    "        if next_url:\n",
    "            soup = BeautifulSoup(requests.get(PREFIX_URL + next_url).text, 'html.parser')\n",
    "            if index:\n",
    "                return collect_recursively(data, soup, tags, next_tag, fcte_name, link, index+1)\n",
    "            else:\n",
    "                return collect_recursively(data, soup, tags, next_tag, fcte_name, link)\n",
    "        else:\n",
    "            return data\n",
    "    except:\n",
    "        pass\n",
    "    return data\n",
    "                \n",
    "def verify_if_treated(soup, tags):\n",
    "    if len(tags) > 4 or len(tags) < 2:\n",
    "        print(\"This case is not treated yet\")\n",
    "        \n",
    "    if len(tags) == 3 or len(tags) == 4:\n",
    "        try:\n",
    "            soup.find(tags[0], {'class': tags[1]})\n",
    "        except:\n",
    "            print(\"This case is not treated yet\")\n",
    "    \n",
    "def collect_all_links(soup, tags, next_tag, fcte_name):      #soup or url? Combine (put verifications also to collect)\n",
    "    verify_if_treated(soup, tags)\n",
    "\n",
    "    data = collect_recursively([], soup, tags, next_tag, fcte_name)      #get rid of 1?\n",
    "    return pd.DataFrame(data).dropna()\n",
    "\n",
    "def collect(forum_df, tags, fcte_name):\n",
    "    data = []\n",
    "    total = len(forum_df['Link'])\n",
    "    index = 0\n",
    "    for url in forum_df['Link']:\n",
    "        index += 1\n",
    "        print('{} out of {}'.format(index, total), end='\\r', flush=True)\n",
    "        #url = forum_df['Link'][0]\n",
    "        soup = BeautifulSoup(requests.get(PREFIX_URL + url).text, 'html.parser')\n",
    "        verify_if_treated(soup, tags)\n",
    "        data.extend(collect_recursively([], soup, tags, next_tag, fcte_name, url))\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Collect all titles and links of the forum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter the 3 parameters needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Holiday Truths\n",
    "#PREFIX_URL = 'https://www.holidaytruths.co.uk/'\n",
    "#START_URL = PREFIX_URL + 'forum/america-canada-discussion-forum-f2-0.html'\n",
    "#soup = BeautifulSoup(requests.get(START_URL).text, 'html.parser')\n",
    "#title1 = 'ESTA question on employment'\n",
    "#title2 = 'Vegas Buffets/Restaurants'\n",
    "\n",
    "#Wrong Planet\n",
    "#PREFIX_URL = 'http://wrongplanet.net/forums'\n",
    "#START_URL = PREFIX_URL + '/viewforum.php?f=19'\n",
    "#soup = BeautifulSoup(requests.get(START_URL).text, 'html.parser')\n",
    "#title1 = 'RE: Kids w/ Classic Autism, PDD-NOS & Speech Delays'\n",
    "#title2 = 'Parents on the spectrum'\n",
    "\n",
    "#Stack Overflow\n",
    "#PREFIX_URL = 'https://stackoverflow.com/'\n",
    "#START_URL = PREFIX_URL + 'questions/tagged/forum'\n",
    "#soup = BeautifulSoup(requests.get(START_URL).text, 'html.parser')\n",
    "#title1 = 'Should DynamoDB adjacency lists use discrete partition keys to model each type of relationship?'\n",
    "#title2 = 'How to Bypass [hide] element in Forums?'\n",
    "\n",
    "#Au Féminin\n",
    "#PREFIX_URL = 'https://astrologie.aufeminin.com/forum'\n",
    "#START_URL = PREFIX_URL + '/all'\n",
    "#soup = BeautifulSoup(requests.get(START_URL).text, 'html.parser')\n",
    "#title1 = 'coucou....'\n",
    "#title2 = 'échange serieux en mp'\n",
    "\n",
    "\n",
    "#Trip Advisor\n",
    "PREFIX_URL = 'https://www.tripadvisor.co.uk/'\n",
    "START_URL = PREFIX_URL + 'ShowForum-g1-i12334-Holiday_Travel.html'\n",
    "soup = BeautifulSoup(requests.get(START_URL).text, 'html.parser')\n",
    "title1 = 'See TOP QUESTIONS before posting!'\n",
    "title2 = 'Use the SEARCH BOX function before posting!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I've found the right tag, it's  ['b', 'a']\n",
      "tag of the next page is not treated in this program\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "try:\n",
    "    right_tags = find_tag(soup.find(text=re.compile(title1)).parent, title2, [])\n",
    "    next_tag = find_next_tag()\n",
    "except:\n",
    "    print(\"Sorry I couldn't find the tags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads = collect_all_links(soup, right_tags, next_tag, collect_forum_data)\n",
    "#print(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Collect all usernames, messages in every link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter the parameters needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_URL = PREFIX_URL + threads.Link[0]\n",
    "soup =  BeautifulSoup(requests.get(START_URL).text, 'html.parser')\n",
    "\n",
    "#Holiday Truths\n",
    "#user1 = 'AnnaM'\n",
    "#user2 = 'Glynis HT Admin'\n",
    "\n",
    "#Wrong Planet\n",
    "#user1 = 'cyberdad'\n",
    "#user2 = 'Solvejg'\n",
    "\n",
    "#Trip Advisor\n",
    "#print(soup.prettify())\n",
    "user1 = 'BradJill'\n",
    "user2 = 'Eden7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I've found the right tag, it's  ['div', 'username', 'a', 'span']\n",
      "tag of the next page is not treated in this program\n"
     ]
    }
   ],
   "source": [
    "right_tags = find_tag(soup.find(text = user1).parent, user2, [])\n",
    "next_tag = find_next_tag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 out of 20\r"
     ]
    }
   ],
   "source": [
    "threads = threads[:21]\n",
    "posts = collect(threads, right_tags, collect_post_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  Link              Username\n",
      "0    ShowTopic-g1-i12334-k7867029-See_TOP_QUESTIONS...              BradJill\n",
      "1    ShowTopic-g1-i12334-k7867029-See_TOP_QUESTIONS...                 Eden7\n",
      "2    ShowTopic-g1-i12334-k7867029-See_TOP_QUESTIONS...              BradJill\n",
      "3    ShowTopic-g1-i12334-k7867031-Use_the_SEARCH_BO...              BradJill\n",
      "4    ShowTopic-g1-i12334-k7867031-Use_the_SEARCH_BO...               KVE1005\n",
      "5    ShowTopic-g1-i12334-k7867031-Use_the_SEARCH_BO...                 Eden7\n",
      "6    ShowTopic-g1-i12334-k5876122-How_to_Use_the_Ho...              BradJill\n",
      "7    ShowTopic-g1-i12334-k5876122-How_to_Use_the_Ho...               Super M\n",
      "8    ShowTopic-g1-i12334-k5876122-How_to_Use_the_Ho...          thegoodwitch\n",
      "9    ShowTopic-g1-i12334-k5876122-How_to_Use_the_Ho...                 Eden7\n",
      "10   ShowTopic-g1-i12334-k11057297-Tui_Uk_website-H...                luxsey\n",
      "11   ShowTopic-g1-i12334-k11057297-Tui_Uk_website-H...             LeyLand10\n",
      "12   ShowTopic-g1-i12334-k11057297-Tui_Uk_website-H...                Toopaz\n",
      "13   ShowTopic-g1-i12334-k11057297-Tui_Uk_website-H...                luxsey\n",
      "14   ShowTopic-g1-i12334-k11057297-Tui_Uk_website-H...                Toopaz\n",
      "15   ShowTopic-g1-i12334-k11057297-Tui_Uk_website-H...                luxsey\n",
      "16   ShowTopic-g1-i12334-k11057297-Tui_Uk_website-H...        traceyandpeter\n",
      "17   ShowTopic-g1-i12334-k11057297-Tui_Uk_website-H...               Cromerp\n",
      "18   ShowTopic-g1-i12334-k11057297-Tui_Uk_website-H...                Toopaz\n",
      "19   ShowTopic-g1-i12334-k11057297-Tui_Uk_website-H...               Cromerp\n",
      "20   ShowTopic-g1-i12334-k11057297-Tui_Uk_website-H...               sunbear\n",
      "21   ShowTopic-g1-i12334-k4827063-Beware_of_Co_op_T...               sunbabe\n",
      "22   ShowTopic-g1-i12334-k4827063-Beware_of_Co_op_T...  BaskervilleArmsHotel\n",
      "23   ShowTopic-g1-i12334-k4827063-Beware_of_Co_op_T...           MashaLondon\n",
      "24   ShowTopic-g1-i12334-k4827063-Beware_of_Co_op_T...                 emgee\n",
      "25   ShowTopic-g1-i12334-k4827063-Beware_of_Co_op_T...             LeyLand10\n",
      "26   ShowTopic-g1-i12334-k4827063-Beware_of_Co_op_T...                 jas t\n",
      "27   ShowTopic-g1-i12334-k4827063-Beware_of_Co_op_T...                 jas t\n",
      "28   ShowTopic-g1-i12334-k4827063-Beware_of_Co_op_T...             LeyLand10\n",
      "29   ShowTopic-g1-i12334-k4827063-Beware_of_Co_op_T...              Graham B\n",
      "..                                                 ...                   ...\n",
      "122  ShowTopic-g1-i12334-k10536532-Olympic_holidays...                kc1705\n",
      "123  ShowTopic-g1-i12334-k9026823-Not_just_travel_h...                 kjb66\n",
      "124  ShowTopic-g1-i12334-k9026823-Not_just_travel_h...              BradJill\n",
      "125  ShowTopic-g1-i12334-k9026823-Not_just_travel_h...              mcpinder\n",
      "126  ShowTopic-g1-i12334-k9026823-Not_just_travel_h...             LeyLand10\n",
      "127  ShowTopic-g1-i12334-k9026823-Not_just_travel_h...              madiwell\n",
      "128  ShowTopic-g1-i12334-k9026823-Not_just_travel_h...             AlanBowen\n",
      "129  ShowTopic-g1-i12334-k9026823-Not_just_travel_h...               Jalyn71\n",
      "130  ShowTopic-g1-i12334-k9026823-Not_just_travel_h...          lloydhankins\n",
      "131  ShowTopic-g1-i12334-k9026823-Not_just_travel_h...               WENDY M\n",
      "132  ShowTopic-g1-i12334-k9026823-Not_just_travel_h...              geoffpfc\n",
      "133  ShowTopic-g1-i12334-k10315509-Re_Question_abou...               Naomi K\n",
      "134  ShowTopic-g1-i12334-k10315509-Re_Question_abou...                4sandi\n",
      "135  ShowTopic-g1-i12334-k10315509-Re_Question_abou...               Naomi K\n",
      "136  ShowTopic-g1-i12334-k10315509-Re_Question_abou...               darsana\n",
      "137  ShowTopic-g1-i12334-k10315509-Re_Question_abou...          waltzsafaris\n",
      "138  ShowTopic-g1-i12334-k10315509-Re_Question_abou...              robbiemp\n",
      "139  ShowTopic-g1-i12334-k10315509-Re_Question_abou...          TheamLoonLau\n",
      "140  ShowTopic-g1-i12334-k10315509-Re_Question_abou...            Lotuspearl\n",
      "141  ShowTopic-g1-i12334-k10315509-Re_Question_abou...            Christof B\n",
      "142  ShowTopic-g1-i12334-k8538588-Cox_and_Kings_tou...              Shruti G\n",
      "143  ShowTopic-g1-i12334-k8538588-Cox_and_Kings_tou...              BradJill\n",
      "144  ShowTopic-g1-i12334-k8538588-Cox_and_Kings_tou...              Roshni K\n",
      "145  ShowTopic-g1-i12334-k8538588-Cox_and_Kings_tou...        186402Tallulah\n",
      "146  ShowTopic-g1-i12334-k8538588-Cox_and_Kings_tou...                 kim l\n",
      "147  ShowTopic-g1-i12334-k8538588-Cox_and_Kings_tou...              Glenda H\n",
      "148  ShowTopic-g1-i12334-k8538588-Cox_and_Kings_tou...             LeyLand10\n",
      "149  ShowTopic-g1-i12334-k8538588-Cox_and_Kings_tou...        186402Tallulah\n",
      "150  ShowTopic-g1-i12334-k8538588-Cox_and_Kings_tou...              Glenda H\n",
      "151  ShowTopic-g1-i12334-k8538588-Cox_and_Kings_tou...              fariba w\n",
      "\n",
      "[152 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
